{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Generation with GPT-2**"
      ],
      "metadata": {
        "id": "UMUklr1K_7D6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzj7JbfN9vnl",
        "outputId": "313a1aca-b36a-4990-c4c0-3c4eac20260e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "prompt = \"Artificial Intelligence is transforming the world because\"\n",
        "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generated Text:\\n\", result[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJRy5CUk-cjm",
        "outputId": "daf950f3-6aa2-4583-a6d1-81ea96708e48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " Artificial Intelligence is transforming the world because of the future. It's about to deliver a world where people are smarter, more capable, and wealthier.\n",
            "\n",
            "And these people are going to have a job and have the opportunity to build a new life. The goal is to give these people a chance to have a better future.\n",
            "\n",
            "The world is changing. Today, more and more people are working less, living in poverty, living with an illness or a disability. And we have more opportunities for those who have the right skills and the skills to learn. And in the 21st century, that's going to be the 21st century.\n",
            "\n",
            "And so, it's going to be our time to make the most of it. We're going to be making a lot of changes and putting a lot of effort into making sure that people can live better lives and have better futures.\n",
            "\n",
            "I want to ask you about something that has occurred to me over the years.\n",
            "\n",
            "You know, I think, I see a lot of people in the media who are just talking about what it is that has changed the way people think about technology. And I think that's a lot of things. And I think, you know, I think that there's a lot of attention being paid to the fact that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion :**\n",
        "\n",
        "This experiment demonstrates the core working principle of Large Language Models (LLMs):\n",
        "\n",
        "LLMs like GPT-2 are trained on huge text datasets.\n",
        "\n",
        "Their goal is not to \"understand\" language in a human way, but to predict the most probable next word (token) based on the input sequence.\n",
        "\n",
        "By repeating this prediction step many times, they generate coherent and contextually relevant sentences.\n",
        "\n",
        "The experiment shows how the same model behaves differently depending on the starting prompt.\n",
        "\n",
        "For example, with “Artificial Intelligence is transforming the world because…”, GPT-2 completes the thought in a logical way, whereas with “In 2050, humans and machines will…”, it predicts a futuristic continuation."
      ],
      "metadata": {
        "id": "dNRJDd2X_X4Z"
      }
    }
  ]
}